{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "import email\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "import itertools\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Enron dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails_df = pd.read_csv('emails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('number of documents in the Enron dataset are:', len(emails_df), 'we will take a subset of this.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a subset of the dataset for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails_df_sub = emails_df.sample(5000, random_state=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails_df_sub['message'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to parse the message column into the respective columns within it. \n",
    "1. Make a new column for each key in the message column\n",
    "2. Split the email addresses \n",
    "3. Extract user name from file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse the emails into a list email objects\n",
    "messages = list(map(email.message_from_string, emails_df_sub['message']))\n",
    "emails_df_sub.drop('message', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "messages[0].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get fields from parsed email objects\n",
    "keys = messages[0].keys()\n",
    "for key in keys:\n",
    "    emails_df_sub[key] = [doc[key] for doc in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse content from emails\n",
    "emails_df_sub['content'] = list(map(get_text_from_email, messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split multiple email addresses\n",
    "emails_df_sub['From'] = emails_df_sub['From'].map(split_email_addresses)\n",
    "emails_df_sub['To'] = emails_df_sub['To'].map(split_email_addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the root of 'file' as 'user'\n",
    "emails_df_sub['user'] = emails_df_sub['file'].map(lambda x:x.split('/')[0])\n",
    "del messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails_df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('shape of the dataframe:', emails_df_sub.shape)\n",
    "# Find number of unique values in each columns\n",
    "for col in emails_df_sub.columns:\n",
    "    print(col, emails_df_sub[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group the emails by the user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails_df_sub_grouped = emails_df_sub.groupby('user')['content'].apply(lambda x: x.sum()).reset_index().groupby('user')['content'].apply(lambda x: x.sum()).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can pre-process the emails by doing the following:\n",
    "1. removing website urls\n",
    "2. standardizing words\n",
    "3. removing puncuation\n",
    "4. remove all numbers\n",
    "5. removing stop words\n",
    "6. removing chat words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "exclude = set(string.punctuation)\n",
    "exclude.update({'\\n', '\\t'})\n",
    "chat_words_lower = sorted(set(w.lower() for w in nltk.corpus.nps_chat.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    try:\n",
    "        doc['content'] = ''.join(''.join(s)[:2] for _, s in itertools.groupby(doc['content']))\n",
    "        doc['content'] = re.sub(r'http\\S+', '', doc['content'])\n",
    "        doc['content'] = ''.join([i for i in doc['content'] if not i.isdigit()])\n",
    "        doc['content'] = ' '.join([i for i in doc['content'].lower().split(' ') if i not in stop])\n",
    "        doc['content'] = ' '.join([i for i in doc['content'].lower().split(' ') if i not in chat_words_lower])\n",
    "        doc['content'] = ''.join(ch for ch in doc['content'] if ch not in exclude)\n",
    "    except:\n",
    "        doc['content'] = ''\n",
    "    return doc['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails_df_sub_grouped['content'] = emails_df_sub_grouped.apply(clean, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails_df_sub_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training = emails_df_sub_grouped.sample(frac=0.7, random_state=0).reset_index(drop=True)\n",
    "testing = emails_df_sub_grouped.loc[~emails_df_sub_grouped.index.isin(training.index)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_corpus = training.values.tolist()\n",
    "testing_corpus = testing.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instantiate\n",
    "vect = CountVectorizer(analyzer='word', ngram_range=(1,2), min_df=0.5, max_df=0.95, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit & transform\n",
    "tf_matrix = vect.fit_transform([content for file, content in training_corpus])\n",
    "\n",
    "print('Number of documents:', tf_matrix.shape[0], ', number of features:', tf_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert sparse matrix to a dense matrix\n",
    "tf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "feature_matrix = pd.DataFrame(tf_matrix.toarray(), columns=vect.get_feature_names())\n",
    "feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying TF_IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of using tf-idf instead of the raw frequencies of occurrence of a\n",
    "    token in a given document is to scale down the impact of tokens that occur\n",
    "    very frequently in a given corpus and that are hence empirically less\n",
    "    informative than features that occur in a small fraction of the training\n",
    "    corpus.\n",
    "    \n",
    " The formula that is used to compute the tf-idf of term t is\n",
    "    tf-idf(d, t) = tf(t) * idf(d, t), and the idf is computed as\n",
    "    idf(d, t) = log [ n / df(d, t) ] + 1 (if ``smooth_idf=False``),\n",
    "    where n is the total number of documents and df(d, t) is the\n",
    "    document frequency; the document frequency is the number of documents d\n",
    "    that contain term t. The effect of adding \"1\" to the idf in the equation\n",
    "    above is that terms with zero idf, i.e., terms  that occur in all documents\n",
    "    in a training set, will not be entirely ignored.\n",
    "    (Note that the idf formula above differs from the standard\n",
    "    textbook notation that defines the idf as\n",
    "    idf(d, t) = log [ n / (df(d, t) + 1) ])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instantiate\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df=0.5, max_df=0.95, stop_words = 'english', norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit & transform\n",
    "tfidf_matrix = tf.fit_transform([content for file, content in training_corpus])\n",
    "\n",
    "print('Number of documents:', tfidf_matrix.shape[0], ', number of features:', tfidf_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert sparse matrix to a dense matrix\n",
    "tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "feature_matrix = pd.DataFrame(tfidf_matrix.toarray(), columns=tf.get_feature_names())\n",
    "feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Similarity using Cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, we are taking an observation outside of the training set (i.e. in the testing set) and t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_corpus[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_email = [testing_corpus[13]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new = tf.transform([content for file, content in new_email])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_similar(tfidf_matrix, new_movie, top_n = 1000):\n",
    "    cosine_similarities = linear_kernel(new_movie, tfidf_matrix).flatten()\n",
    "    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1]]\n",
    "    return [(index, cosine_similarities[index]) for index in related_docs_indices][0:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similarity = pd.DataFrame()\n",
    "for index, score in find_similar(tfidf_matrix, new):\n",
    "    similarity = similarity.append(pd.DataFrame({'similarity_score':score, 'person':training_corpus[index][0]}, index=[0]), ignore_index=True)\n",
    "similarity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.foxnews.com/story/2004/07/08/fast-facts-key-enron-players.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
